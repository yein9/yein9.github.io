---
title:  "Machine Learning Crush Course : Generalization"
excerpt: "."

categories:
  - ML
tags:
  - Machine Learning Crush Course
  - Google
  

---

[Machine Learning Crash Course : Generalization](https://developers.google.com/machine-learning/crash-course/generalization/video-lecture?hl=en)

## Generalization : Peril of Overfiting



<img src="https://developers.google.com/machine-learning/crash-course/images/GeneralizationA.png" alt="figure 1" style="zoom: 67%;" />

There are orange and blue dots in the picture above. And imagine a good model that can divide the blues from the oranges. 

<img src="https://developers.google.com/machine-learning/crash-course/images/GeneralizationB.png" alt="figure 2" style="zoom: 67%;" />

This picture is may be the best model for these dots. 

##### Loss low, but still a bad model?

<img src="https://developers.google.com/machine-learning/crash-course/images/GeneralizationC.png" alt="figure 3" style="zoom:67%;" />

 Figure3 shows what happens when new data are added to the model. It turned out that the model adapted very poorly to the new data. Much of data are in the wrong categoty. The model shown in Figure2 and Figure 3 **overfits the pecularities of the data it trained on**. An overfit model gets a low loss during the training however, it is very bad at predicting new data. This kind of overfitting is caused by making a model more complex than it needs. **The fundamental tension of machine learing is between fitting our data well, but also fitting data as simply as possible.** 

 Machine learning's goal is to predict well on new data from **(hidden) true probability distribution(?)**. However, the model can only gets sample from training data set. 

 William of Ockham, a 14th century friar and philosopher, loved simplicity. He believed that scientists should prefer simpler formulas or theories over more complex ones. To put Ockham's razor in machine learning terms.

> **The less complex an ML model**, the more likely that a good empirical result is not just due to the peculiarities of the sample.

 This Ockham's razor formalized into the fields of statistical and computational learning theory. These fields have developed **generalization bounds**--a statistical description of a model's ability to generalize to new data based on factors such as:

- the complexity of the model
- the model's performance on training data

Divide your data set into 2 subsets if you want your machine learning model making good prediction to unseen data.

- training set - a subset to train a model
- test set - a subset to test the model

For good performance, the test set should be large enough and don't reuse the same data set over and over.



#### ML details

The following three basic assumptions guide generalization

1.  We draw examples independently and identically at random from the distribution; examples doesn't influence each other.
2.  The distribution is stationary; the distribution doesn't change within data set.
3.  We draw examples from patitions from the same distribution.

